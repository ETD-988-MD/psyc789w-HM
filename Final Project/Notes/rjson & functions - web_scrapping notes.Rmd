---
title: "json & scrapping functions - Web-Scrapping Approach"
date: "January 17, 2015"
output: html_document
---
```{r,results="hide",echo=F}
#Packages
lapply(c("dplyr","foreign","reshape2","ggplot2","rjson","XML","RCurl"),library,character.only=T) 
#quick way to load multiple packages
```

The following will outline my attempts at web-scrapping. Below the `json`, `XML`, and `RCurl` packages will be utilized to scrap data from the web. I heavily used an online tutorial based of a Cambridge class that can be found [here]("http://www.r-bloggers.com/web-scraping-the-basics/").

### rjson approach for simple webscrapping

See the tutorial I was working off 

This is a way to use paste with URLs
```{r}
var=201301
url = paste("http://stats.grok.se/json/en/",var,"/web_scrapping",sep="")
url
#browseURL(url)
raw.data <- readLines(url,warn=F)
```

The data on the sight is in a json format (Java Script). There is package that allows R to process this kind of material.

```{r}
require(rjson)
rd  <- fromJSON(raw.data)
rd.views <- rd$daily_views
df <- as.data.frame(unlist(rd.views))
```

Plotting this data
```{r}
require(ggplot2)
require(lubridate)
df$date <-  as.Date(rownames(df))
colnames(df) <- c("views","date")
ggplot(df,aes(date,views))+
  geom_line()+
  geom_smooth()+
  theme_bw(base_size=20)
```

Using a loop to specify specific URLs (by altering the date.)
```{r}
url <- data.frame(url=NA)
for (year in 2012:2013){
  for (month in 1:9){
  	url[month,] <- paste("http://stats.grok.se/json/en/",year,0,month,"/web_scraping",sep="")
	}
	for (month in 10:12){
		url[month,] <- paste("http://stats.grok.se/json/en/",year,month,"/web_scraping",sep="")
	}
}
head(url)
```

Here is small function for simple web-scrapping.
```{r}
getData <- function(url){
  for(i in 1:nrow(url)){
    raw.data <- readLines(url[i,], warn=F) 
    rd  <- fromJSON(raw.data)
    rd.views <- rd$daily_views 
    rd.views <- unlist(rd.views)
    rd <- as.data.frame(rd.views)
    rd$date <- rownames(rd)
    rownames(rd) <- NULL
    assign(paste("rd",i,sep="_"),rd) 
  }
  data <- data.frame()
  for(i in 1:nrow(url)){
    data <- rbind(get(paste("rd",i,sep="_")),data)
    }
  data
}
web.data <- getData(url)
dim(web.data) 
head(web.data)
```

Plotting this data
```{r}
web.data$date <-  as.Date(web.data$date)
ggplot(web.data,aes(x=date,y=rd.views)) +
  geom_line()+
  geom_smooth()+
  theme_bw()
```

This is a "simple" web-scrapping methodology because it's retrieving from a (relatively) easy data structure, and something is known about the data that we are compiling. For this reason, this function works when retrieving this specific data set. However, we can expand on this later on and use some of the functionality located here down the road. 

### Building a "scrapper"

Specifically pulling from part 2 of the [Cambridge Tutorial]("http://quantifyingmemory.blogspot.com/2014/02/web-scraping-part2-digging-deeper.html") The code from the previous tutorial left off here. 

```{r}
require(rjson)
url  <- "http://stats.grok.se/json/en/201201/web_scraping"
raw.data <- readLines(url, warn="F") 
rd  <- fromJSON(raw.data)
rd.views <- rd$daily_views 
rd.views <- unlist(rd.views)
rd <- as.data.frame(rd.views)
rd$date <- rownames(rd)
rownames(rd) <- NULL
head(rd)
```

The information is scrapped from the sight -- though the tutorial left off with the best way to set up a more sustainable scrape that doesn't require as much effort to get set up. 

#### Using functions to ease the process

```{r}
getData <- function(url){
  require(rjson)
  raw.data <- readLines(url, warn="F") 
  rd  <- fromJSON(raw.data)
  rd.views <- rd$daily_views 
  rd.views <- unlist(rd.views)
  rd <- as.data.frame(rd.views)
  rd$date <- rownames(rd)
  rownames(rd) <- NULL
  rd$date <- as.Date(rd$date)
  return(rd)
}

getData("http://stats.grok.se/json/en/201201/web_scraping") %>% head(.)
```

Building a function can mechanize the process (as long as you account for the parts that might vary).

#### Creating URL Function

This function seeks to alter the temporal pieces of the URL in order to create a vector of urls contained within a specified time period.

```{r}
getUrl <- function(y1,y2,term){
  root <- "http://stats.grok.se/json/en/"
    urls <- NULL
    for (year in y1:y2){
      for (month in 1:9){
        urls <- c(urls,(paste(root,year,0,month,"/",term,sep="")))
      }
    
      for (month in 10:12){
        urls <- c(urls,(paste(root,year,month,"/",term,sep="")))
    	}
    }
    return(urls)
}

#Easy way to construct the URL
urls <- getUrl(y1=2013,y2=2014,"Euromaidan")
urls
```

Now get data for each of them and store that data

```{r}
results=NULL
for (url in urls){
  results <- rbind(results,getData(url))
}
head(results)

ggplot(tail(results,100),aes(date,rd.views))+geom_line(color="blue")#+theme_bw()
```

This can be powerful way to retrieve large amounts of information. 

### Downloading the web page

#### Getting to know HTML structure

Let's look at this webpage: [http://en.wikipedia.org/wiki/Euromaidan]("http://en.wikipedia.org/wiki/Euromaidan")

- Headings
- Images
- links
- references
- tables

To look at the code (in Google Chrome), right-click somewhere on the page and select **'inspect element'**.

Tree-structure (parents, siblings)

**Back to Wikipedia**

HTML tags.

They come in pairs and are surrounded by these guys:
`<>`

e.g. a heading might look like this:

\<h1\>MY HEADING\</h1\>
<h1>MY HEADING</h1>

Which others do you know or can you find?

**HTML tags**

- \<html>: starts html code
- \<head> : contains meta data etc
- \<script> : e.g. javascript to be loaded
- \<style> : css code
- \<meta> : denotes document properties, e.g. author, keywords
- \<title> : 
- \<body> : 

**HTML tags2**

- \<div>, \<span> :these are used to break up a document into sections and boxes
- \<h1>,\<h2>,\<h3>,\<h4>,\<h5> Different levels of heading
- \<p> : paragraph
- \<br> : line break
- and others: \<a>, \<ul>, \<tbody>, \<th>, \<td>, \<ul>, \<ul>, <img>

**Principles of scraping**

- Identify the tag
- Download the web-page
- Extract content matching the tag
- Save the content
- Optional: repeat

**Download the web page**

`XML Parser` Parses an XML or HTML file or string containing XML/HTML content, and generates an R structure representing the XML/HTML tree. 

```{r}
require(RCurl)
require(XML)

url <- "http://en.wikipedia.org/wiki/Euromaidan"
SOURCE <-  getURL(url,encoding="UTF-8") #Download the page. 
#Note that this function is part of RCurl and not what we made above.

#this is a very very long line. Let's not print it. Instead:
substring(SOURCE,1,200) #Way of extracting pieces of the string, since the whole file is very large
PARSED <- htmlParse(SOURCE) #Format the html code
```

we can use XPath expressions to extract elements from HTML.

```{r}
xpathSApply(PARSED, "//h1")
```

Not so pretty. But! Specifying `xmlValue` strips away the surrounding code and returns only the content of the tag

```{r}
xpathSApply(PARSED, "//h1",xmlValue)

#See how low you can go. Remember, it's a hierarchy
xpathSApply(PARSED, "//h2",xmlValue)

head(xpathSApply(PARSED, "//h3",xmlValue))

# and links
length(xpathSApply(PARSED, "//a/@href"))
# there's loads of them. We need to be more selective
```

### CSS and Xpath

web-designers use Cascading Style Sheets to determine the way a webpage looks. Like variables: change the style, rather than the every item on a page. <strong>CSS allows us to make better selections, by latching onto tags</strong>.**Xpath allows us to move up and down the html tree structure**. CSS can be an html **attribute**. Tree-structure is navigated a bit like that on your computer (c:/windows/system)

Here is an example of getting the references:
```{r}
head(xpathSApply(PARSED, "//span[@class='citation news']",xmlValue),1)

#without xmlValue
head(xpathSApply(PARSED, "//span[@class='citation news']/a/@href"),1)
```

```{r}
links <- (xpathSApply(PARSED, "//span[@class='citation news']/a/@href"))
browseURL(links[1]) #This is pretty neat. It takes you right to the linked page.
```

>Note: the `browseURL()` function take you right to a specified URL.

#### Fundamental XPath Syntax

- /      Select from the root
- //     Select anywhere in document
- @      Select attributes. Use in square brackets

In this example, we select all elements of 'span'
...Which have an **attribute** "class" of the value "citation news"
...then we select all links
...and return all attributes labeled "href" (the urls)

e.g. `..."//span[@class='citation news']/a/@href"`

Like in R, we use square brackets to make selections. Example: `head(xpathSApply(PARSED, "//span[@class='citation news'][17]/a/@href"))`

We can also use "wildcards" (i.e. fuzzy identifiers that don't demand a specific classification).
- * selects any node or tag
- @* selects any attribute (used to define nodes)

```{r}
(xpathSApply(PARSED, "//*[@class='citation news'][17]/a/@href"))
(xpathSApply(PARSED, "//span[@class='citation news'][17]/a/@*"))
```

You can use functions, e.g. for **partial matches**. This is useful if there are subtle variations within or between pages.
```{r eval=F}
head(xpathSApply(PARSED, "//span[starts-with(@class,'citation')][17]/a/@href"))
head(xpathSApply(PARSED, "//span[contains(@class,'citation')][17]/a/@href"))
```

**_Example syntax_**
`[function(attribute,string)]`

#### BBC Example

```{r}
url <- "http://www.bbc.co.uk/news/world-europe-26333587"
SOURCE <- getURL(url,encoding="UTF-8")
PARSED <- htmlParse(SOURCE)
xpathSApply(PARSED, "//h1[@class='story-header']",xmlValue)

bbcScraper <- function(url){
  SOURCE <-  getURL(url,encoding="UTF-8")
  PARSED <- htmlParse(SOURCE)
  title=(xpathSApply(PARSED, "//h1[@class='story-header']",xmlValue))
  date=as.character(xpathSApply(PARSED, "//meta[@name='OriginalPublicationDate']/@content"))
  return(c(title,date))
}
bbcScraper("http://www.bbc.co.uk/news/world-middle-east-26333533")
bbcScraper("http://www.bbc.co.uk/sport/0/football/26332893")

#issue with the retrieving all the meta-data (the date in the above case)
bbcScraper2 <- function(url){
  title=date=NA #Return empty values in case field not found
  SOURCE <-  getURL(url,encoding="UTF-8") 
  PARSED <- htmlParse(SOURCE)
  title=(xpathSApply(PARSED, "//h1[@class='story-header']",xmlValue))
  date=(xpathSApply(PARSED, "//meta[@name='OriginalPublicationDate']/@content"))
  if (is.null(date)){
    date=(xpathSApply(PARSED, "//span[@class='date']",xmlValue))
  }
  return(c(title,as.character(date)))
}
bbcScraper2("http://www.bbc.co.uk/sport/0/football/26332893")
```

Here is where I try and build my own scrapper - see practice file. 

Next Lesson ("Week 3")
-----------------------

See the [link]("http://quantifyingmemory.blogspot.com/2014/03/web-scraping-scaling-up-digital-data.html").

Here we are trying to scale this process up so that we can scrape multiple urls simultaneously. 
```{r}
#Here is the code for the bbcScrapper from last time. 
bbcScraper <- function(url){
  require(RCurl)
  require(XML)
  SOURCE <-  getURL(url,encoding="UTF-8")
  PARSED <- htmlParse(SOURCE,encoding="UTF-8")
  title=xpathSApply(PARSED, "//h1[@class='story-header']",xmlValue)
  date=as.character(xpathSApply(PARSED, "//meta[@name='OriginalPublicationDate']/@content"))
  if (is.null(date))    date <- NA
  if (is.null(title))    title <- NA
  return(c(title,date))
}

urls <- c("http://www.bbc.co.uk/news/business-26414285","http://www.bbc.co.uk/news/uk-26407840","http://www.bbc.co.uk/news/world-asia-26413101","http://www.bbc.co.uk/news/uk-england-york-north-yorkshire-26413963")
results=NULL
for(url in urls){
  newEntry <- bbcScraper(url)
  results <- rbind(results,newEntry)
}
data.frame(results) #the results are stored as a matrix
```

The disadvantage of looping is that we stor the data in an inefficient way.
```{r}
temp = NULL
temp <- rbind(temp,results[1,])
temp <- rbind(temp,results[2,])
temp <- rbind(temp,results[3,])
temp #This is a matrix
```

In each case we are copying the whole table in order to add a single line. This is (above all things) slow and we need to keep two copied in memory (means we can only ever use at most half of computer's RAM).

A more efficient way is to use `sapply()`.
```{r}
#Urls again
urls <- c("http://www.bbc.co.uk/news/business-26414285","http://www.bbc.co.uk/news/uk-26407840","http://www.bbc.co.uk/news/world-asia-26413101","http://www.bbc.co.uk/news/uk-england-york-north-yorkshire-26413963")
sapply(urls,bbcScraper)
```

And `plyr` can be used here -- note that this is old code that needs to be revisited in lue of `dplyr` -- the `ldply()` function essentially runs a function across a list and then returns a dataframe, which is super useful given what we're doing. 
```{r}
require(plyr)
dat <- ldply(urls,bbcScraper) 
dat
```

#### Link Harvesting

Entering URLs by hand is tedious. We can speed this up by automating the collection of links. How can we find relevant links?

* Scraping URLs in a search result
* those on the front page

Running a search on Nigeria using bbc returns the following url `http://www.bbc.co.uk/search?v2=true&page=1&q=Russia#page=4`. Make sure you run through to the next page so that you get the apgination pattern. 
```{r}
#Retrieving all the links from the search
url<-"http://www.bbc.co.uk/search?v2=true&page=1&q=Russia#page=4"
SOURCE <-  getURL(url,encoding="UTF-8")
PARSED <- htmlParse(SOURCE)
xpathSApply(PARSED, "//a/@href")
```

Here we need to partial out only unique links
```{r}
unique(xpathSApply(PARSED, "//ol[2]/li[1]/article/div/h1/a/@href"))
unique(xpathSApply(PARSED, "//*[@id='orb-modules']/section[3]/ol[1]/li[1]/article/div"))
unique(xpathSApply(PARSED,"//section[3]/ol/li/a/@href")) 

#This provides us with all the links...we only need the news related ones. 
require(plyr)
targets <- unique(xpathSApply(PARSED,"//section/ol/li/article/a/@href"))
targets[1:5]
results <- ldply(targets[1:5],bbcScraper) #only the first 5 links
results
```

Here I will run a scrapper that accounts for pagination. 

```{r}
bbcSearchURL <- function(search.url,pages){
  urls <- list()
  urls[1] <- search.url
  for(i in 2:pages){
    urls[i] <- paste0(search.url,"#page=",i)
    }
  return(urls)
}
url <- "http://www.bbc.co.uk/search?q=Russia&sa_f=search-serp"
urls <- bbcSearchURL(url,4)
urls

#Let's build this into the BBC scraper
bbcScraper2 <- function(url,pages=NULL){
  require(RCurl)
  require(XML)
  if(!is.null(pages)){
    bbcSearchURL <- function(url,pages){
      urls <- NA
      urls[1] <- url
      for(i in 2:pages){
        urls[i] <- paste0(url,"#page=",i)
        }
      return(urls)
      }
    urls <- bbcSearchURL(url,pages)
    results <- data.frame(title=NA,date=NA)
    for(i in 1:length(urls)){
      temp.url <- urls[i]
      SOURCE <-  getURL(temp.url,encoding="UTF-8")
      PARSED <- htmlParse(SOURCE,encoding="UTF-8")
      results$title[i]<- xpathSApply(PARSED, "//h1[@class='story-header']",xmlValue)
      results$date[i]<- as.character(xpathSApply(PARSED, "//meta[@name='OriginalPublicationDate']/@content"))
      if (is.null(date))    results$date[i] <- NA
      if (is.null(title))    results$title[i] <- NA
    }
    } else {
      results <- data.frame(title=NA,date=NA)
      SOURCE <-  getURL(url,encoding="UTF-8")
      PARSED <- htmlParse(SOURCE,encoding="UTF-8")
      results$title <- xpathSApply(PARSED, "//h1[@class='story-header']",xmlValue)
      results$date <- as.character(xpathSApply(PARSED, "//meta[@name='OriginalPublicationDate']/@content"))
      if (is.null(date))    results$date <- NA
      if (is.null(title))    results$title <- NA
    }
  return(results)
  }

url <- "http://www.bbc.co.uk/search?q=Russia&sa_f=search-serp"
bbcScraper2(url,pages=NULL)
##THIS IS STILL IN DEVELOPMENT
```

### Automating downloads

As with the newspaper articles, downloads are facilitated by getting the right links. Let's search for pdf files and downlod the first ten results:
```{r}
url <- "http://lib.ru/GrepSearch?Search=pdf" #So "free" Russian lit site

SOURCE <-  getURL(url,encoding="UTF-8") # Specify encoding when dealing with non-latin characters
PARSED <- htmlParse(SOURCE)
links <- (xpathSApply(PARSED, "//a/@href"))
links[grep("pdf",links)][1]
```

Note: anything that is downloaded will be saved in the working directory. 

```{r,eval=F}
#Making the link the appropriate format
links <- paste0("http://lib.ru",links[grep("pdf",links)])
links[1]

#Now the the first 3 PDFs
require(stringr)
for (i in 1:3){
  parts <- unlist(str_split(links[i],"/"))
  outName <- parts[length(parts)]
  print(outName)
  download.file(links[i],outName)
}
```

The material has been downloaded directly into my working directory. Though if this process were to be implemented a greater level of precision and checks would have to follow, since the process is time intensive. 

### Using APIs

From Wikipedia:

> With Ajax, web applications can send data to, and retrieve data from, a server asynchronously (in the background) without interfering with the display and behavior of the existing page. Data can be retrieved using the XMLHttpRequest object. Despite the name, the use of XML is not required (JSON is often used instead.

How does this work?

- You load the webpage
- There is a script in the code.
- As well as placeholders, empty fields
- The script runs, and executes the Ajax call. 
- This connects, in this case, with the Facebook API
- the API returns data about the page from Facebook's servers
- The JQuery syntax interprets the JSON and fills the blanks in the html
- The user sees the number of shares.

Problem: when we download the page, we see only the first three steps. 
Solution: intercept the Ajax call, or, go straight to the source

Here is the working example used for retrieving the number of shares and likes from Facebook via a news article. 

```{r}
fqlQuery='select share_count,like_count,comment_count from link_stat where url="'
url="http://www.theguardian.com/world/2014/mar/03/ukraine-navy-officers-defect-russian-crimea-berezovsky"
queryUrl = paste0('http://graph.facebook.com/fql?q=',fqlQuery,url,'"')  #ignoring the callback part
lookUp <- URLencode(queryUrl) #What do you think this does?
lookUp
```

Pasting this into the URL returns the following.

```
{
   "data": [
      {
         "share_count": 390,
         "like_count": 428,
         "comment_count": 232
      }
   ]
}
```

This process (purportedly) works for any URL. Here the author provides another example. 
```{r}
require(rjson)
url="http://quantifyingmemory.blogspot.com/2014/02/web-scraping-basics.html"
queryUrl = paste0('http://graph.facebook.com/fql?q=',fqlQuery,url,'"')  #ignoring the callback part
lookUp <- URLencode(queryUrl)
rd <- readLines(lookUp, warn="F") 
dat <- fromJSON(rd)
dat

#Grabbing numbers from the list
dat$data[[1]]$like_count;dat$data[[1]]$share_count;dat$data[[1]]$comment_count
```

APIs are really useful: we don't (normally!) have to worry about terms of use

Next Lesson ("Week 4")
-----------------------

This 4th and final lesson can be found [here]("http://quantifyingmemory.blogspot.com/2014/03/web-scraping-working-with-apis.html")

### Digital data collection

Goals:

* Devise means of accessing data
* Retrieve that data
* Tabulate and store the data

APIs allow applications to communicate with each other. Examples of this would be:

- Amazon API allows web-sites to link directly to products - up-to-date prices, option to buy
- Buying stuff online: verification of credit-card data
- Smartphone apps: e.g. for accessing Twitter
- Maps with location data, e.g. Yelp
- Share content between social networking sites
- Embed videos
- Log in via Facebook

Wikipedia:

> When used in the context of web development, an API is typically defined as a set of Hypertext Transfer Protocol (HTTP) request messages, along with a definition of the structure of response messages, which is usually in an Extensible Markup Language (XML) or JavaScript Object Notation (JSON) format.

- Out: HTTP request
- In: JSON or XML 

We can handle both processes through R.

Using the code from above:
```{r}
fqlQuery='select share_count,like_count,comment_count from link_stat where url="'
url="http://www.theguardian.com/world/2014/mar/03/ukraine-navy-officers-defect-russian-crimea-berezovsky"
queryUrl = paste0('http://graph.facebook.com/fql?q=',fqlQuery,url,'"')  #ignoring the callback part
lookUp <- URLencode(queryUrl) 
lookUp
require(rjson)
rd <- readLines(lookUp, warn="F") 
dat <- fromJSON(rd)
dat
```

This works, but how do we make it all work together in a more fluid way?

Here is the process:

First, **Find an API**

  - See [http://www.programmableweb.com/apis/directory]("http://www.programmableweb.com/apis/directory") for a usable api directory.

Let's use Google map (See J. Gonzalez's work on [utilizing the Google Maps api with R]("http://www.jose-gonzalez.org/using-google-maps-api-r/#.VMAQJy7F_NR") . This could be really useful!)

Wrapper functions in package `dismo` (also explore packages ggmaps, maptools)

Two APIs of interest:

1. **geo location**
  - See [https://developers.google.com/maps/documentation/geocoding/]("https://developers.google.com/maps/documentation/geocoding/")
    - required parameters
      - address [place]
      - sensor [=false]
    - options: bounds,key,language, region
  -Query:
    - See [https://maps.googleapis.com/maps/api/geocode/json?]("https://maps.googleapis.com/maps/api/geocode/json?")
    - address=Kremlin,Moscow
    - sensor=false
    - separate with '&'
    - E.g. `https://maps.googleapis.com/maps/api/geocode/json?address=Kremlin,Moscow&sensor=false`

Writing a function for this material.

```{r}
getUrl <- function(address,sensor = "false") {
 root <- "http://maps.google.com/maps/api/geocode/json?"
 u <- paste0(root,"address=", address, "&sensor=false")
 return(URLencode(u))
}
getUrl("Kremlin, Moscow")
```


Now putting this to use.
```{r}
require(RJSONIO)
target <- getUrl("Kremlin, Moscow")
dat <- fromJSON(target)
latitude <- dat$results[[1]]$geometry$location["lat"]
longitude <- dat$results[[1]]$geometry$location["lng"]
place <- dat$results[[1]]$formatted_address
latitude;longitude;place
```

These geo codes can be **EXTREMELY** useful when creating maps from existing datasets. 
    
2. **static maps**

See [here]("https://developers.google.com/maps/documentation/staticmaps/") for the nitty gritty via Google.

What you need to account for:

* base="http://maps.googleapis.com/maps/api/staticmap?center=""
* center= latitude (e.g 55.75), longitude (e.g. 37.62)
  + OR: centre =place (Kremlin, Moscow)
* zoom (1= zoomed right out, 18 zoomed right in)
* maptype="hybrid" #satellite, hybrid, terrain, roadmap
* suffix = "&size=800x800&sensor=false&format=png""

Example:
`http://maps.googleapis.com/maps/api/staticmap?center=55.75,37.62&zoom=13&maptype=hybrid&size=800x800&sensor=false&format=png`

Constructing this type of URL in R using paste
```{r}
base="http://maps.googleapis.com/maps/api/staticmap?center="
latitude=55.75
longitude=37.62
zoom=13
maptype="hybrid"
suffix ="&size=800x800&sensor=false&format=png"
target <- paste0(base,latitude,",",longitude,
                 "&zoom=",zoom,"&maptype=",maptype,suffix)
target
```

Next we download the map
```{r,eval=F}
download.file(target,"test.png",mode="wb")
```

### Other ways of harvesting APIs

#### Twitter

Works much like the Facebook API

```{r}
url="http://www.theguardian.com/uk-news/2014/mar/10/rise-zero-hours-contracts"
target=paste0("http://urls.api.twitter.com/1/urls/count.json?url=",url)
rd <- readLines(target, warn="F") 
dat <- fromJSON(rd)
shares <- dat$count
shares
```


#### Yandex Maps

Yandex maps (and retrieving information from them) is very similar to Google maps. See [here]("http://api.yandex.com/maps/doc/staticapi/1.x/dg/concepts/input_params.xml") for the details. USe the code from above to harvest this information. 

#### YouTube

See [here]("https://developers.google.com/youtube/2.0/developers_guide_protocol_audience") for the details.

Important pieces of the puzzle to keep in mind: 

* **Video stats** (id = video ID, e.g. “Ya2elsR5s5s”): url=paste0(“https://gdata.youtube.com/feeds/api/videos/”,id,“?v=2&alt=json”)

* **Comments** (id = video ID, e.g. “Ya2elsR5s5s”): url=paste0(“http://gdata.youtube.com/feeds/api/videos/”,id,“/comments?v=2&alt=json”)

* **Search** (ukrainian protests): url=“https://gdata.youtube.com/feeds/api/videos?q=ukrainian+protests&alt=json”

Application:
```{r}
require(dplyr)
require(stringr)
require(lubridate)
id <- "T5ENwwozALc" #video id
target=paste0("http://gdata.youtube.com/feeds/api/videos/",id,"?v=2&alt=json")
rd <- readLines(target,warn="F") 
dat <- fromJSON(rd)
ave.rating <- dat$entry$`gd$rating`$average
views <- dat$entry$`yt$statistics`[[2]]
length <- dat$entry$`media$group`$`media$thumbnail`[[4]]$time
title <- dat$entry$title
published <- unlist(str_split(dat$entry$published,"T"))[1] %>% as.Date()

#FUNCTION: pulling video stats
YouTubeStats <- function(id){
  require(dplyr)
  require(stringr)
  require(lubridate)
  target=paste0("http://gdata.youtube.com/feeds/api/videos/",id,"?v=2&alt=json")
  rd <- readLines(target,warn="F") 
  dat <- fromJSON(rd)
  ave.rating <- dat$entry$`gd$rating`$average
  views <- dat$entry$`yt$statistics`[[2]]
  length <- dat$entry$`media$group`$`media$thumbnail`[[4]]$time
  title <- dat$entry$title
  published <- unlist(str_split(dat$entry$published,"T"))[1] %>% as.Date()
  data <- data.frame(title,published,length,views,ave.rating)
  return(data)
}
YouTubeStats("3mjpjbOVo-I") #Different URL
#Works!

```


### Social APIs

```{r}
#Linkedin
url="http://www.theguardian.com/uk-news/2014/mar/10/rise-zero-hours-contracts"
target=paste0("http://www.linkedin.com/countserv/count/share?url=$",url,"&format=json")
  rd <- readLines(target, warn="F") 
  dat <- fromJSON(rd)

#StumbleUpon
url="http://www.theguardian.com/uk-news/2014/mar/10/rise-zero-hours-contracts"
target=paste0("http://www.stumbleupon.com/services/1.01/badge.getinfo?url=",url)
  rd <- readLines(target, warn="F") 
  dat <- fromJSON(rd)

```




