---
title: "json & scrapping functions - Web-Scrapping Approach"
date: "January 17, 2015"
output: html_document
---
```{r,results="hide",echo=F}
#Packages
lapply(c("dplyr","foreign","reshape2","ggplot2","rjson","XML","RCurl"),library,character.only=T) 
#quick way to load multiple packages
```

The following will outline my attempts at web-scrapping. Below the `json`, `XML`, and `RCurl` packages will be utilized to scrap data from the web. I heavily used an online tutorial based of a Cambridge class that can be found [here]("http://www.r-bloggers.com/web-scraping-the-basics/").

### rjson approach for simple webscrapping

See the tutorial I was working off 

This is a way to use paste with URLs
```{r}
var=201301
url = paste("http://stats.grok.se/json/en/",var,"/web_scrapping",sep="")
url
#browseURL(url)
raw.data <- readLines(url,warn=F)
```

The data on the sight is in a json format (Java Script). There is package that allows R to process this kind of material.

```{r}
require(rjson)
rd  <- fromJSON(raw.data)
rd.views <- rd$daily_views
df <- as.data.frame(unlist(rd.views))
```

Plotting this data
```{r}
require(ggplot2)
require(lubridate)
df$date <-  as.Date(rownames(df))
colnames(df) <- c("views","date")
ggplot(df,aes(date,views))+
  geom_line()+
  geom_smooth()+
  theme_bw(base_size=20)
```

Using a loop to specify specific URLs (by altering the date.)
```{r}
url <- data.frame(url=NA)
for (year in 2012:2013){
  for (month in 1:9){
  	url[month,] <- paste("http://stats.grok.se/json/en/",year,0,month,"/web_scraping",sep="")
	}
	for (month in 10:12){
		url[month,] <- paste("http://stats.grok.se/json/en/",year,month,"/web_scraping",sep="")
	}
}
head(url)
```

Here is small function for simple web-scrapping.
```{r}
getData <- function(url){
  for(i in 1:nrow(url)){
    raw.data <- readLines(url[i,], warn=F) 
    rd  <- fromJSON(raw.data)
    rd.views <- rd$daily_views 
    rd.views <- unlist(rd.views)
    rd <- as.data.frame(rd.views)
    rd$date <- rownames(rd)
    rownames(rd) <- NULL
    assign(paste("rd",i,sep="_"),rd) 
  }
  data <- data.frame()
  for(i in 1:nrow(url)){
    data <- rbind(get(paste("rd",i,sep="_")),data)
    }
  data
}
web.data <- getData(url)
dim(web.data) 
head(web.data)
```

Plotting this data
```{r}
web.data$date <-  as.Date(web.data$date)
ggplot(web.data,aes(x=date,y=rd.views)) +
  geom_line()+
  geom_smooth()+
  theme_bw()
```

This is a "simple" web-scrapping methodology because it's retrieving from a (relatively) easy data structure, and something is known about the data that we are compiling. For this reason, this function works when retrieving this specific data set. However, we can expand on this later on and use some of the functionality located here down the road. 

### Building a "scrapper"

Specifically pulling from part 2 of the [Cambridge Tutorial]("http://quantifyingmemory.blogspot.com/2014/02/web-scraping-part2-digging-deeper.html") The code from the previous tutorial left off here. 

```{r}
require(rjson)
url  <- "http://stats.grok.se/json/en/201201/web_scraping"
raw.data <- readLines(url, warn="F") 
rd  <- fromJSON(raw.data)
rd.views <- rd$daily_views 
rd.views <- unlist(rd.views)
rd <- as.data.frame(rd.views)
rd$date <- rownames(rd)
rownames(rd) <- NULL
head(rd)
```

The information is scrapped from the sight -- though the tutorial left off with the best way to set up a more sustainable scrape that doesn't require as much effort to get set up. 

#### Using functions to ease the process

```{r}
getData <- function(url){
  require(rjson)
  raw.data <- readLines(url, warn="F") 
  rd  <- fromJSON(raw.data)
  rd.views <- rd$daily_views 
  rd.views <- unlist(rd.views)
  rd <- as.data.frame(rd.views)
  rd$date <- rownames(rd)
  rownames(rd) <- NULL
  rd$date <- as.Date(rd$date)
  return(rd)
}

getData("http://stats.grok.se/json/en/201201/web_scraping") %>% head(.)
```

Building a function can mechanize the process (as long as you account for the parts that might vary).

#### Creating URL Function

This function seeks to alter the temporal pieces of the URL in order to create a vector of urls contained within a specified time period.

```{r}
getUrl <- function(y1,y2,term){
  root <- "http://stats.grok.se/json/en/"
    urls <- NULL
    for (year in y1:y2){
      for (month in 1:9){
        urls <- c(urls,(paste(root,year,0,month,"/",term,sep="")))
      }
    
      for (month in 10:12){
        urls <- c(urls,(paste(root,year,month,"/",term,sep="")))
    	}
    }
    return(urls)
}

#Easy way to construct the URL
urls <- getUrl(y1=2013,y2=2014,"Euromaidan")
urls
```

Now get data for each of them and store that data

```{r}
results=NULL
for (url in urls){
  results <- rbind(results,getData(url))
}
head(results)

ggplot(tail(results,100),aes(date,rd.views))+geom_line(color="blue")#+theme_bw()
```

This can be powerful way to retrieve large amounts of information. 

### Downloading the web page

#### Getting to know HTML structure

Let's look at this webpage: [http://en.wikipedia.org/wiki/Euromaidan]("http://en.wikipedia.org/wiki/Euromaidan")

- Headings
- Images
- links
- references
- tables

To look at the code (in Google Chrome), right-click somewhere on the page and select **'inspect element'**.

Tree-structure (parents, siblings)

**Back to Wikipedia**

HTML tags.

They come in pairs and are surrounded by these guys:
`<>`

e.g. a heading might look like this:

\<h1\>MY HEADING\</h1\>
<h1>MY HEADING</h1>

Which others do you know or can you find?

**HTML tags**

- \<html>: starts html code
- \<head> : contains meta data etc
- \<script> : e.g. javascript to be loaded
- \<style> : css code
- \<meta> : denotes document properties, e.g. author, keywords
- \<title> : 
- \<body> : 

**HTML tags2**

- \<div>, \<span> :these are used to break up a document into sections and boxes
- \<h1>,\<h2>,\<h3>,\<h4>,\<h5> Different levels of heading
- \<p> : paragraph
- \<br> : line break
- and others: \<a>, \<ul>, \<tbody>, \<th>, \<td>, \<ul>, \<ul>, <img>

**Principles of scraping**

- Identify the tag
- Download the web-page
- Extract content matching the tag
- Save the content
- Optional: repeat

**Download the web page**

`XML Parser` Parses an XML or HTML file or string containing XML/HTML content, and generates an R structure representing the XML/HTML tree. 

```{r}
require(RCurl)
require(XML)

url <- "http://en.wikipedia.org/wiki/Euromaidan"
SOURCE <-  getURL(url,encoding="UTF-8") #Download the page. 
#Note that this function is part of RCurl and not what we made above.

#this is a very very long line. Let's not print it. Instead:
substring(SOURCE,1,200) #Way of extracting pieces of the string, since the whole file is very large
PARSED <- htmlParse(SOURCE) #Format the html code
```

we can use XPath expressions to extract elements from HTML.

```{r}
xpathSApply(PARSED, "//h1")
```

Not so pretty. But! Specifying `xmlValue` strips away the surrounding code and returns only the content of the tag

```{r}
xpathSApply(PARSED, "//h1",xmlValue)

#See how low you can go. Remember, it's a hierarchy
xpathSApply(PARSED, "//h2",xmlValue)

head(xpathSApply(PARSED, "//h3",xmlValue))

# and links
length(xpathSApply(PARSED, "//a/@href"))
# there's loads of them. We need to be more selective
```

### CSS and Xpath

web-designers use Cascading Style Sheets to determine the way a webpage looks. Like variables: change the style, rather than the every item on a page. <strong>CSS allows us to make better selections, by latching onto tags</strong>.**Xpath allows us to move up and down the html tree structure**. CSS can be an html **attribute**. Tree-structure is navigated a bit like that on your computer (c:/windows/system)

Here is an example of getting the references:
```{r}
head(xpathSApply(PARSED, "//span[@class='citation news']",xmlValue),1)

#without xmlValue
head(xpathSApply(PARSED, "//span[@class='citation news']/a/@href"),1)
```

```{r}
links <- (xpathSApply(PARSED, "//span[@class='citation news']/a/@href"))
browseURL(links[1]) #This is pretty neat. It takes you right to the linked page.
```

>Note: the `browseURL()` function take you right to a specified URL.

#### Fundamental XPath Syntax

- /      Select from the root
- //     Select anywhere in document
- @      Select attributes. Use in square brackets

In this example, we select all elements of 'span'
...Which have an **attribute** "class" of the value "citation news"
...then we select all links
...and return all attributes labeled "href" (the urls)

e.g. `..."//span[@class='citation news']/a/@href"`

Like in R, we use square brackets to make selections. Example: `head(xpathSApply(PARSED, "//span[@class='citation news'][17]/a/@href"))`

We can also use "wildcards" (i.e. fuzzy identifiers that don't demand a specific classification).
- * selects any node or tag
- @* selects any attribute (used to define nodes)

```{r}
(xpathSApply(PARSED, "//*[@class='citation news'][17]/a/@href"))
(xpathSApply(PARSED, "//span[@class='citation news'][17]/a/@*"))
```

You can use functions, e.g. for **partial matches**. This is useful if there are subtle variations within or between pages.
```{r eval=F}
head(xpathSApply(PARSED, "//span[starts-with(@class,'citation')][17]/a/@href"))
head(xpathSApply(PARSED, "//span[contains(@class,'citation')][17]/a/@href"))
```

**_Example syntax_**
`[function(attribute,string)]`

#### BBC Example

```{r}
url <- "http://www.bbc.co.uk/news/world-europe-26333587"
SOURCE <- getURL(url,encoding="UTF-8")
PARSED <- htmlParse(SOURCE)
xpathSApply(PARSED, "//h1[@class='story-header']",xmlValue)

bbcScraper <- function(url){
  SOURCE <-  getURL(url,encoding="UTF-8")
  PARSED <- htmlParse(SOURCE)
  title=(xpathSApply(PARSED, "//h1[@class='story-header']",xmlValue))
  date=as.character(xpathSApply(PARSED, "//meta[@name='OriginalPublicationDate']/@content"))
  return(c(title,date))
}
bbcScraper("http://www.bbc.co.uk/news/world-middle-east-26333533")
bbcScraper("http://www.bbc.co.uk/sport/0/football/26332893")

#issue with the retrieving all the meta-data (the date in the above case)
bbcScraper2 <- function(url){
  title=date=NA #Return empty values in case field not found
  SOURCE <-  getURL(url,encoding="UTF-8") 
  PARSED <- htmlParse(SOURCE)
  title=(xpathSApply(PARSED, "//h1[@class='story-header']",xmlValue))
  date=(xpathSApply(PARSED, "//meta[@name='OriginalPublicationDate']/@content"))
  if (is.null(date)){
    date=(xpathSApply(PARSED, "//span[@class='date']",xmlValue))
  }
  return(c(title,as.character(date)))
}
bbcScraper2("http://www.bbc.co.uk/sport/0/football/26332893")
```

Here is where I try and build my own scrapper - see practice file. 

### Next Lesson ("Week 3")

See the [link]("http://quantifyingmemory.blogspot.com/2014/03/web-scraping-scaling-up-digital-data.html").

Here we are trying to scale this process up so that we can scrape multiple urls simultaneously. 
```{r}
#Here is the code for the bbcScrapper from last time. 
bbcScraper <- function(url){
  require(RCurl)
  require(XML)
  SOURCE <-  getURL(url,encoding="UTF-8")
  PARSED <- htmlParse(SOURCE,encoding="UTF-8")
  title=xpathSApply(PARSED, "//h1[@class='story-header']",xmlValue)
  date=as.character(xpathSApply(PARSED, "//meta[@name='OriginalPublicationDate']/@content"))
  if (is.null(date))    date <- NA
  if (is.null(title))    title <- NA
  return(c(title,date))
}

urls <- c("http://www.bbc.co.uk/news/business-26414285","http://www.bbc.co.uk/news/uk-26407840","http://www.bbc.co.uk/news/world-asia-26413101","http://www.bbc.co.uk/news/uk-england-york-north-yorkshire-26413963")
results=NULL
for(url in urls){
  newEntry <- bbcScraper(url)
  results <- rbind(results,newEntry)
}
data.frame(results) #the results are stored as a matrix
```

The disadvantage of looping is that we stor the data in an inefficient way.
```{r}
temp = NULL
temp <- rbind(temp,results[1,])
temp <- rbind(temp,results[2,])
temp <- rbind(temp,results[3,])
temp #This is a matrix
```

In each case we are copying the whole table in order to add a single line. This is (above all things) slow and we need to keep two copied in memory (means we can only ever use at most half of computer's RAM).

A more efficient way is to use `sapply()`.
```{r}
#Urls again
urls <- c("http://www.bbc.co.uk/news/business-26414285","http://www.bbc.co.uk/news/uk-26407840","http://www.bbc.co.uk/news/world-asia-26413101","http://www.bbc.co.uk/news/uk-england-york-north-yorkshire-26413963")
sapply(urls,bbcScraper)
```

And `plyr` can be used here -- note that this is old code that needs to be revisited in lue of `dplyr` -- the `ldply()` function essentially runs a function across a list and then returns a dataframe, which is super useful given what we're doing. 
```{r}
require(plyr)
dat <- ldply(urls,bbcScraper) 
dat
```

#### Link Harvesting

Entering URLs by hand is tedious. We can speed this up by automating the collection of links. How can we find relevant links?

* Scraping URLs in a search result
* those on the front page

Running a search on Nigeria using bbc returns the following url `http://www.bbc.co.uk/search?q=Nigeria`.
```{r}
#Retrieving all the links from the search
url<-"http://www.bbc.co.uk/search?q=Russia&sa_f=search-serp"
SOURCE <-  getURL(url,encoding="UTF-8")
PARSED <- htmlParse(SOURCE)
xpathSApply(PARSED, "//a/@href")
```

Here we need to partial out only unique links
```{r}
unique(xpathSApply(PARSED,"//a/@href")) 
unique(xpathSApply(PARSED, "//*[@id='orb-modules']"))

#This provides us with all the links...we only need the news related ones. 
targets <- unique(xpathSApply(PARSED,"//a[@id='orb-modules']"))
results <- ldply(targets[1:5],bbcScraper) #only the first 5 pages
results
```















