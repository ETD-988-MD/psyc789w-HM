---
title: "BBC News Scraper - Walk through on Building the Function"
author: "Eric Dunford - PSYC789W - R-Programming"
output: html_document
---

The goal is to build a function that generates a data frame of all the top stories of some subject. The user should be capable of running queries within the function and be able to view the stories relevance by the degree of social media activity the story received. 

### Initial Approach
```{r}
require(rjson)
require(RCurl)
require(XML)
url <- "http://www.bbc.com/news/world-europe-30878406"
raw.data <- getURL(url,encoding="UTF-8")
parsed.data <- htmlParse(raw.data)

xpathSApply(parsed.data,"//h1",xmlValue) #title
xpathSApply(parsed.data,
            "//*[@id='main-content']/div[2]/div[1]/span[1]/span[1]",
            xmlValue) #publishing date
xpathSApply(parsed.data,"//*[@id='main-content']/div[2]/div[1]/p",xmlValue) #story
```

Let's see if this pattern holds across other bbc stories.
```{r}
url <- "http://www.bbc.com/news/world-asia-30902237"
raw.data <- getURL(url,encoding="UTF-8")
parsed.data <- htmlParse(raw.data)

xpathSApply(parsed.data,"//h1",xmlValue) #title
xpathSApply(parsed.data,
            "//*[@id='main-content']/div[2]/div[1]/span[1]/span[1]",
            xmlValue) #publishing date
xpathSApply(parsed.data,"//*[@id='main-content']/div[2]/div[1]/p",xmlValue) #story
```

It worked! So now we need a loop to find all the stories for a particular subject.

```{r}
url <- "http://www.bbc.com/news/world-asia-30902237"

#Webscrapper just for title and date of story
bbcScrapper <- function(url){
  require(RCurl)
  require(XML)
  raw.data <- getURL(url,encoding="UTF-8")
  parsed.data <- htmlParse(raw.data)
  story.title <- xpathSApply(parsed.data,"//h1",xmlValue) 
  story.date <- xpathSApply(parsed.data,
                            "//*[@id='main-content']/div[2]/div[1]/span[1]/span[1]",
                            xmlValue) 
  c(story.title,story.date)
  }
bbcScrapper("http://www.bbc.com/news/science-environment-30888767")
```

Let's step this up. Here I will scan three BBC articles and put them into a data frame. 

```{r}
urls <- c("http://www.bbc.com/news/world-us-canada-30908095","http://www.bbc.com/news/world-europe-30907443","http://www.bbc.com/news/science-environment-30888767")

#Expanding the function to deal for when date is NULL
bbcScrapper <- function(url){
  require(RCurl)
  require(XML)
  raw.data <- getURL(url,encoding="UTF-8")
  parsed.data <- htmlParse(raw.data)
  story.title <- xpathSApply(parsed.data,"//h1",xmlValue) 
  story.date <- xpathSApply(parsed.data,
                            "//*[@id='main-content']/div[2]/div[1]/span[1]/span[1]",
                            xmlValue) 
  if (is.null(story.date)){
    story.date <- xpathSApply(parsed.data, "//span[@class='date']",xmlValue)
  }
  return(c(story.title,as.character(story.date)))
  }
bbc <- sapply(urls,bbcScrapper) %>% data.frame
head(bbc)

#Another more efficient approach...if not a little archaic.
require(plyr)
bbc <- ldply(urls,bbcScrapper)
bbc
```

### Taking an `rvest` approach

```{r}
url <- "http://www.bbc.com/news/world-europe-30878406"
raw.data <- getURL(url,encoding="UTF-8")
parsed.data <- htmlParse(raw.data)

require(rvest)
url <- "http://www.bbc.com/news/world-australia-30945303"
raw.data <- xml(url)
date <- xml_node(raw.data,".date") %>% html_text()
title <- xml_node(raw.data,".story-header") %>% html_text()
date;title 
class(date)

bbcScraper <- function(url){
  raw.data <- html(url)
  date <- html_node(raw.data,".date") %>% html_text()
  title <- html_node(raw.data,".story-header") %>% html_text()
  c(title,date) 
}
bbcScraper("http://www.bbc.com/news/world-us-canada-30908095")

#Faster application
require(plyr)
urls <- c("http://www.bbc.com/news/world-us-canada-30908095","http://www.bbc.com/news/world-europe-30907443","http://www.bbc.com/news/science-environment-30888767")
test <- ldply(urls,bbcScraper) 
test
```

Here I've built the same kind of scraper, but in a more simplistic form (less searching through HTML code)

Retrieving links form a search page.
```{r}
require(XML)
url<-"http://www.bbc.co.uk/search?v2=true&page=1&q=Russia#page=4"
pg <- htmlParse(url)
links <- pg %>% html_nodes(".search-results article") %>% html_nodes("a") %>% html_attr("href") %>% unique %>% as.list
#Programs have an interactive format that doesn't work for our purposes. So I'm getting rid of it,
links <- gsub("programmes",NA,links) 
links <- links[!is.na(links)]
```

And then harvesting the data from them

```{r,eval=F}
test2 <- ldply(links[4],bbcScraper) 
```

But there are issues. I need a scraper that can work around NAs

```{r}
bbcScraper <- function(url){
  require(rvest)
  require(dplyr)
  if(!is.character(url)){
    return("Issue: Non-character value detected")
    } else{
      raw.data <- html(url)
      date <- html_node(raw.data,".date") %>% html_text()
      if(is.null(html_node(raw.data,".story-header"))){
        title <- html_node(raw.data,".story-body__h1") %>% html_text() 
      } else {
        title <- html_node(raw.data,".story-header") %>% html_text() 
        }
      }
  if(!is.null(title) & !is.null(data)){
    c(title,date)
    }
  }
bbcScraper("http://www.bbc.com/news/world-europe-17839672")
bbcScraper(NA)
bbcScraper(1)
```

Not a great repair, but let's see if it does the trick.
```{r}
test2 <- ldply(links,bbcScraper) 
head(test2)
```
The scrapper isn't perfect. But for now (for these sets of links), it works.

### BBC Search Engine Function
Now, let's build a master function where one can search, harvest the search links, and then get the title and publishing date of the story.

```{r}
search <- "http://www.bbc.co.uk/search?q=" #search part of the URL
query <- "Nigeria"
url <- paste0(search,query)
pg <- htmlParse(url)
links <- pg %>% html_nodes(".search-results article") %>% html_nodes("a") %>% html_attr("href") %>% unique %>% as.list
links <- gsub("programmes",NA,links) 
links <- links[!is.na(links)]
links
```

------
Worked! But we need it to scan the links from every possible search page (or at least 4 pages back or so)
```{r}
#The best solution might just be a loop. 
search <- "http://www.bbc.co.uk/search?q=" #search part of the URL
query <- "Nigeria"
for(i in 2:100){ #page 1 follows a different structure
  url <- paste0(search,query,"#page=",20)
  pg <- htmlParse(url)
  links <- pg %>% html_nodes(".search-results article") %>% html_nodes("a") %>% html_attr("href")  %>% unique %>% as.list
  links <- gsub("programmes",NA,links) 
  links <- links[!is.na(links)]
  assign(paste0("links",i),links)
  }
```
Interesting...peeling across the pages doesn't really return unique results. I guess I won't move forward with that right now. 

Attempting to "Jump" to the next page. I'm not a 100% the underlying functionality of this portion of the rvest package. 
```{r}
p <- html("http://www.bbc.co.uk/search?q=Nigeria")
"http://www.bbc.co.uk/search?q=Nigeria" %>% html_session %>% jump_to() %>% html_nodes(".search-results article") %>% html_nodes("a") %>% html_attr("href")
```
------

Now let's build this into a search function

```{r}
bbcResearch <- function(query){
  require(dplyr)
  url <- paste0("http://www.bbc.co.uk/search?q=",query)
  links <- htmlParse(url) %>% html_nodes(".search-results article") %>% html_nodes("a") %>% html_attr("href") %>% unique %>% as.list
  links <- gsub("programmes",NA,links)
  links <- links[!is.na(links)]
  return(links)
}
bbcResearch("Cancer")
bbcResearch("Hoffman")
bbcResearch("University of Maryland")
```

### Assessing how "popular a story is" - Relevancy

Accessing the Facebook API to see how much these articles were shared.
```{r}
require(rjson)
fqlQuery='select share_count,comment_count,like_count, total_count from link_stat where url="'
url=links[1]
queryUrl = paste0('http://graph.facebook.com/fql?q=',fqlQuery,url,'"')  #ignoring the callback part
lookUp <- URLencode(queryUrl) 
rd <- readLines(lookUp, warn="F") 
data <- fromJSON(rd)
data.frame(shares=data$data[[1]]$share_count,No.of.Comments=data$data[[1]]$comment_count,No.of.Likes=data$data[[1]]$like_count,total=data$data[[1]]$total_count)
```

This works. Let's turn it into a function.
```{r}
Relevance <- function(url){
  require(rjson)
  queryUrl = paste0('http://graph.facebook.com/fql?q=','select share_count,comment_count,like_count, total_count from link_stat where url="',url,'"') 
  lookUp <- URLencode(queryUrl) 
  rd <- readLines(lookUp, warn="F")
  data <- fromJSON(rd)
  output <- data.frame(shares=data$data[[1]]$share_count,No.of.Comments=data$data[[1]]$comment_count,No.of.Likes=data$data[[1]]$like_count,total=data$data[[1]]$total_count)
  return(output)
}
Relevance("http://www.bbc.co.uk/news/uk-scotland-glasgow-west-30934937")
Relevance("http://www.bbc.co.uk/news/magazine-30583512")
ldply(links,Relevance) # good - handles higher processing
```

Great! An easy way to assess the relevancy of a story (as per social media activity).

### Master function

Now I'll put it all together.

```{r}
bbcStoryScrape <- function(query){
  require(dplyr)
  require(rvest)
  require(rjson)
  require(XML)
  require(lubridate)
  require(plyr) #I know: having the two siblings together is a 'no no', but I need `ldply()`. So I'll live dangerously
  url <- paste0("http://www.bbc.co.uk/search?q=",query)
  links <- htmlParse(url) %>% html_nodes(".search-results article") %>% html_nodes("a") %>% html_attr("href") %>% unique %>% as.list
  links <- gsub("programmes",NA,links)
  links <- gsub("music",NA,links)
  links <- gsub("blogs",NA,links)
  links <- links[!is.na(links)]
  bbcScraper <- function(url){
    if(!is.character(url)){
      return("Issue: Non-character value detected")
      } else{
        raw.data <- html(url)
        if(is.null(html_node(raw.data,".date"))){
          date <- html_node(raw.data,"td span") %>% html_text() %>% strtrim(.,width=35) %>% gsub("Last Updated:\\s\\w+,\\s","",.)
          } else{
            date <- html_node(raw.data,".date") %>% html_text()
            }}
    if(is.null(html_node(raw.data,".story-header")) & is.null(html_node(raw.data,".headlinestory b")) & !is.null(html_node(raw.data,".sh"))){
      title <- html_node(raw.data,".sh") %>% html_text()
      } else {if(is.null(html_node(raw.data,".story-header")) & is.null(html_node(raw.data,".headlinestory b"))){
        title <- html_node(raw.data,".story-body__h1") %>% html_text() 
        } else{if(!is.null(html_node(raw.data,".story-header")) & is.null(html_node(raw.data,".headlinestory b"))){
          title <- html_node(raw.data,".story-header") %>% html_text() 
          } else{
            title <- html_node(raw.data,".headlinestory b") %>% html_text() 
            }
          }
        }
    if(!is.null(title) & !is.null(data)){
      c(title,date)
      }
    }
  data <- ldply(links,bbcScraper)
  colnames(data) <- c("Headline","Date_Published")
  Relevance <- function(url){
    queryUrl = paste0('http://graph.facebook.com/fql?q=','select share_count,comment_count,like_count, total_count from link_stat where url="',url,'"') 
    lookUp <- URLencode(queryUrl) 
    rd <- readLines(lookUp, warn="F")
    data <- fromJSON(rd)
    output <- data.frame(Shares=data$data[[1]]$share_count,No.of.Comments=data$data[[1]]$comment_count,No.of.Likes=data$data[[1]]$like_count,Total=data$data[[1]]$total_count)
    return(output)
    }
  SM_data <- ldply(links,Relevance)
  output <- cbind(data,SM_data)
  output$link <- links
  #Arrange the output in descending order
  output <- arrange(output,desc(Total))
  #Date formating
  output$Date_Published <- output$Date_Published %>% as.Date(.,"%d %B %Y")
  output
  }
temp <- bbcStoryScrape("Saudi Arabia")
temp
```

All the pieces appear to be working well together. That said, the function seems to be having difficulty dealing with old stories (around the turn of the century) when the structure of the BBC website was much different. This requires manual tweaking, which I might do later down the line, but thought was unnecessary at the moment. 

Also, the function doesn't discriminate by language; thus, it's capable of pulling news stories in other languages, which can present date issues. 
