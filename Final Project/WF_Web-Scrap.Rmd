---
title: "Web-Scrapping - Varied Approaches"
author: "Eric Dunford"
date: "January 17, 2015"
output: html_document
---
```{r,results="hide",echo=F}
#Packages
lapply(c("dplyr","foreign","reshape2"),library,character.only=T) 
#quick way to load multiple packages
```

The following will outline my attempts at web-scrapping. 

### rjson approach for simple webscrapping

See the tutorial I was working off [here](http://www.r-bloggers.com/web-scraping-the-basics/)

This is a way to use paste with URLs
```{r}
var=201301
url = paste("http://stats.grok.se/json/en/",var,"/web_scrapping",sep="")
url
#browseURL(url)
raw.data <- readLines(url,warn=F)
```

The data on the sight is in a json format. There is package that allows R to process this kind of material.

```{r}
require(rjson)
rd  <- fromJSON(raw.data)
rd.views <- rd$daily_views
df <- as.data.frame(unlist(rd.views))
```

Plotting this data
```{r}
require(ggplot2)
require(lubridate)
df$date <-  as.Date(rownames(df))
colnames(df) <- c("views","date")
ggplot(df,aes(date,views))+
  geom_line()+
  geom_smooth()+
  theme_bw(base_size=20)
```

Using a loop to specify specific URLs (by altering the date.)
```{r}
url <- data.frame(url=NA)
for (year in 2012:2013){
  for (month in 1:9){
  	url[month,] <- paste("http://stats.grok.se/json/en/",year,0,month,"/web_scraping",sep="")
	}
	for (month in 10:12){
		url[month,] <- paste("http://stats.grok.se/json/en/",year,month,"/web_scraping",sep="")
	}
}
head(url)
```

Here is small function for simple web-scrapping.
```{r}
getData <- function(url){
  for(i in 1:nrow(url)){
    raw.data <- readLines(url[i,], warn=F) 
    rd  <- fromJSON(raw.data)
    rd.views <- rd$daily_views 
    rd.views <- unlist(rd.views)
    rd <- as.data.frame(rd.views)
    rd$date <- rownames(rd)
    rownames(rd) <- NULL
    assign(paste("rd",i,sep="_"),rd) 
  }
  data <- data.frame()
  for(i in 1:nrow(url)){
    data <- rbind(get(paste("rd",i,sep="_")),data)
    }
  data
}
web.data <- getData(url)
dim(web.data) 
head(web.data)
```

Plotting this data
```{r}
web.data$date <-  as.Date(web.data$date)
ggplot(web.data,aes(x=date,y=rd.views)) +
  geom_line()+
  geom_smooth()+
  theme_bw()
```

This is a "simple" web-scrapping methodology because it's retrieving from a (relatively) easy data structure, and something is known about the data that we are compiling. For this reason, this function works when retrieving this specific data set. However, we can expand on this later on and use some of the functionality located here down the road. 

### the "rvest" package approach

`rvest` is a hadley package - play on "harvest". See [rvest - introduction](http://blog.rstudio.org/2014/11/24/rvest-easy-web-scraping-with-r/)

```{r}
require(rvest)
#For specifics on selector gadget to find the css of the website, see
#vignette("selectorgadget")

#To see capability, demo the package
#demo(package = "rvest")
require(dplyr) #for the pipe
```

Let's give it a swing pulling data similar to that of the vignette (i.e. IMDB movie data on the movie "Boyhood")
```{r}
boyhood.url <- html("http://www.imdb.com/title/tt1065073/fullcredits")
cast <- html_nodes(boyhood.url,"span.itemprop")
length(cast)
df <- html_text(cast) %>% data.frame(cast=.)
df$cast <- as.character(df$cast)
head(df)
#This has a list of all the actors in the movie
```

This was a simple exercise...though there are real limitations to this approach. One thing is that the selectorgadge can be a rather tedious thing to use. And even then you have to manually adapt it when you want to retrieve different pieces of information.

Here I will play with some of the versatility of the package
```{r,eval=F}
boyhood.url <- "http://www.imdb.com/title/tt1065073/"

#Here is a way to view html pages in Rstudio.
require(httpuv)
rstudio::viewer(boyhood.url)
#htm1_session(),jump_to(), follow_link(), back(), and forward()
html_session(boyhood.url) %>% follow_link("see full cast & crew")
```

### 'XML' package approach - for extracting tables from the web

See [XML walkthrough - Columbia](http://www.columbia.edu/~cjd11/charles_dimaggio/DIRE/styled-4/styled-6/code-13/)

For this initial example we will be pulling from the [following URL]("http://apps.saferoutesinfo.org/project_list/results.cfm")
```{r}
library(XML)
projects<-htmlParse("http://apps.saferoutesinfo.org/project_list/results.cfm")
class(projects) #weird document class
projects.table<- readHTMLTable(projects,stringsAsFactors = FALSE)
str(projects.table) #structure of the table
expenditures<-data.frame(cbind(projects.table[[1]][1],projects.table[[1]][3])) #partialing out the arrays we want
#And like that the data is in a workable format in R.

#Cleaning the data
colnames(expenditures)[2] <- "Award.Amount"
expenditures$Award.Amount<-gsub(",", "", as.character(expenditures$Award.Amount), fixed = TRUE) #Resolving a formating issue with the amount column
expenditures$Award.Amount<-as.numeric(substring(expenditures$Award.Amount,2))
expenditures$Award.Amount[1:5]
expenditures <- expenditures %>% group_by(State) %>% mutate(u=mean(Award.Amount),sd=sd(Award.Amount))

#Top Awar winners.
top5 <- expenditures %>% group_by(State) %>% summarize(Award.Amount=mean(Award.Amount)) 
str(top5)
top5 <- arrange(top5,desc(Award.Amount))
head(top5,5)
```

Here is code for when there are multiple tables on a page. The example uses the following [Tornado]("http://www.cdc.gov/mmwr/preview/mmwrhtml/mm6128a3.htm?s_cid=mm6128a3_e%0d%0a")
```{r,}
tornados<-htmlParse("http://www.cdc.gov/mmwr/preview/mmwrhtml/mm6128a3.htm?s_cid=mm6128a3_e%0d%0a")
tornado.tables<- readHTMLTable(tornados,stringsAsFactors = FALSE)
length(tornado.tables)

#Here are the different tables that have been drawn from. 
head(tornado.tables[[1]])  
tail(tornado.tables[[1]])

head(tornado.tables[[2]])  
tail(tornado.tables[[2]])

head(tornado.tables[[3]])  
tail(tornado.tables[[3]]) 
```

You can loop through the multiple tables and extract the pieces of data that you want. 

### Building a "scrapper"

Back to the [Cambridge Tutorials]("http://quantifyingmemory.blogspot.com/2014/02/web-scraping-part2-digging-deeper.html") The code from the previous tutorial left off here. 

```{r}
require(rjson)
url  <- "http://stats.grok.se/json/en/201201/web_scraping"
raw.data <- readLines(url, warn="F") 
rd  <- fromJSON(raw.data)
rd.views <- rd$daily_views 
rd.views <- unlist(rd.views)
rd <- as.data.frame(rd.views)
rd$date <- rownames(rd)
rownames(rd) <- NULL
head(rd)
```

The information is scrapped from the sight -- though the tutorial left off with the best way to set up a more sustainable scrape that doesn't require as much effort to get set up. 

#### Using functions to ease the process

```{r}
getData <- function(url){
  require(rjson)
  raw.data <- readLines(url, warn="F") 
  rd  <- fromJSON(raw.data)
  rd.views <- rd$daily_views 
  rd.views <- unlist(rd.views)
  rd <- as.data.frame(rd.views)
  rd$date <- rownames(rd)
  rownames(rd) <- NULL
  rd$date <- as.Date(rd$date)
  return(rd)
}

getData("http://stats.grok.se/json/en/201201/web_scraping") %>% head(.)
```

Building a function can mechanize the process (as long as you account for the parts that might vary).

#### Creating the URL
```{r}
getUrls <- function(y1,y2,term){
  root <- "http://stats.grok.se/json/en/"
    urls <- NULL
    for (year in y1:y2){
      for (month in 1:9){
        urls <- c(urls,(paste(root,year,0,month,"/",term,sep="")))
      }
    
      for (month in 10:12){
      	urls <- c(urls,(paste(root,year,month,"/",term,sep="")))
    	}
    }
    return(urls)
}
```










