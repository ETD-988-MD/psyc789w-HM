---
title: "Untitled"
output: html_document
---
### Quick take aways

* `try(.,silent=T)` function allows you to process an error, return nothing, and then go on with the loop. But it will still give you back the error results if assigned to an object. 

Preliminary scan on the site material that Michael is looking to extract from Foreign Ministry sites. 

## Resources 

* CIA's "Chiefs of State and Cabinet Members of Foreign Governments" -- list can be found [here]("https://www.cia.gov/library/publications/world-leaders-1/index.html"). 
    + Note: the html cycle works off a two letter country code. html is straightforward. 

### Chief of State and Cabinet Scraper

First, to build the simple version just focusing on Afghanistan. 
```{r}
require(rvest)
require(dplyr)
url <- "https://www.cia.gov/library/publications/world-leaders-1/"
request <- paste0(url,"AF",".html")
raw.data <- html(request)
title <- html_nodes(raw.data,".title span") %>% html_text
leaders <- html_nodes(raw.data,".cos_name span") %>% html_text
leader.data <- data.frame("Country"="AF","Office"=title,"office.holder"=leaders)
head(leader.data)
```

Now, to mechanize it.

First, we need to find the correct coding procedure that matches up to the site's url. 
```{r}
# library(countrycode)
# require(stringr)
# countrycode("Afghanistan","country.name","iso2c")
# #Appears correct now in reverse.
# countrycode("AF","iso2c","country.name")
# countrycode("AD","fips104","country.name") #doesn't match ups with andorra

#website using FIPS codeing - need to extract this code structure
fibs.raw <- html("http://www.cloford.com/resources/codes/index.htm")
countries <- html_nodes(fibs.raw,".outlinetable td:nth-child(3)") %>% html_text
fibs <- html_nodes(fibs.raw,"p+ .outlinetable td:nth-child(5)") %>% html_text
#Manual correction of mismatches with CIA website
# fibs <- fibs[-4] # American Samoa
# fibs <- fibs[-6] # Anguilla
# fibs <- fibs[-27] # British Virgin Islands
# fibs <- fibs[-35] # Cayman Islands
```

We need an array containing every country in the world. 
```{r}
# #country list from dept of state. 
# WLraw <- html("http://www.state.gov/misc/list/")
# country.list <- html_nodes(WLraw,"blockquote a") %>% html_text
# #No convert into the iso2c
# country <- countrycode(country.list,"country.name","iso2c") %>% data.frame 
# colnames(country) <- "iso2c"
# country <- arrange(country,iso2c)
# head(country)
```

Now the application -- loop approach
```{r}
#leader.data <- data.frame(country=NA,office=NA,office.holder=NA)
url <- "https://www.cia.gov/library/publications/world-leaders-1/"
for(i in 1:length(fibs)){
  request <- paste0(url,fibs[i],".html")
  if(!inherits(try(html(request),silent=T), 'try-error')){
    raw.data <- html(request)
    title <- html_nodes(raw.data,".title span") %>% html_text
    leaders <- html_nodes(raw.data,".cos_name span") %>% html_text
    leader.data <- data.frame("Country"=fibs[i],"Office"=title,"office.holder"=leaders)
    } else{next}
  }    
leader.data %>% nrow
head(leader.data)
```




-----

## The Scrape
First, there are preliminary analytical tools that need to be studied further. 

### Web_Scraping (Building a More Efficient Scraper)

The following code is from [here]("http://stackoverflow.com/questions/24576962/how-write-code-to-web-crawling-and-scraping-in-r")...see the answer

```{r}
library(XML)
library(httr)
url <- "http://www.wikiart.org/en/claude-monet/mode/all-paintings-by-alphabet/"
hrefs <- list()
for (i in 1:23) {
  response <- GET(paste0(url,i))
  doc      <- content(response,type="text/html")
  hrefs    <- c(hrefs,doc["//p[@class='pb5']/a/@href"])
}
url      <- "http://www.wikiart.org"
xPath    <- c(pictureName = "//h1[@itemprop='name']",
              date        = "//span[@itemprop='dateCreated']",
              author      = "//a[@itemprop='author']",
              style       = "//span[@itemprop='style']",
              genre       = "//span[@itemprop='genre']")
get.picture <- function(href) {
  response <- GET(paste0(url,href))
  doc      <- content(response,type="text/html")
  info     <- sapply(xPath,function(xp)ifelse(length(doc[xp])==0,NA,xmlValue(doc[xp][[1]])))
}
pictures <- do.call(rbind,lapply(hrefs,get.picture))
head(pictures)
```



### Scraping with Python
First, [this]("http://docs.python-guide.org/en/latest/scenarios/scrape/") and [this]("http://learnpythonthehardway.org/book/ex5.html") for starters. 

### Text Mining
See [this]("http://cran.r-project.org/web/packages/tm/tm.pdf")


